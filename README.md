# Practical Deep Learning Tips, Tricks, and Pitfalls

Here we will go through some deep learning practicalities which catch people (or at least me!) by surprise, and are good to know in general

## Loss functions, tasks, and output activation functions are related!

We will list the task, the loss functions you may want to use, and what activation function you use on the output layer

### Numerical Prediction

### Binary Classification

### Multiclass Classification

## Relu and dropout...

dropout after relu activation = bad! (talk about probabilities)

## Batch size...
Talk about batch size as well as relation to learning rate

## How many hidden nodes should I put in a layer
tbd

## How big should I make my network
tbd
